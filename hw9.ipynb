{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT2j-dwiSTmC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roguxivlo/machine-learning-24L/blob/main/hw9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Homework Assignment - *Do Androids Dream of Electric Sheep?***\n",
        "\n",
        "-------------------------------------  \n",
        "\n",
        "\"Do Androids Dream of Electric Sheep?\" – the famous title of Philip K. Dick’s novel – raises a fascinating question: if artificial intelligence could dream, what would it see?  \n",
        "\n",
        "In this assignment, we explore a phenomenon known as **neural network dreams**, where instead of optimizing a neural network's weights, we **optimize the input itself** to achieve a desired classification outcome. Given a fully trained MNIST classification network, your goal is to manipulate its inputs so that it confidently predicts each digit from 0 to 9, starting from pure noise.  \n",
        "\n",
        "## **Tasks Description**  \n",
        "\n",
        "During this class we designed and trained a **MNIST classification neural network**, which takes a **batch of grayscale images** of size **$28 \\times 28$** as input and outputs a probability distribution over the 10 digit classes (0–9). However, instead of using real MNIST images, you will **treat the input batch itself as a set of trainable parameters** and optimize it so that the network classifies each image as a specific digit.  \n",
        "\n",
        "1. Your first task is to generate **a batch of 10 images**, where each image is\n",
        "   classified as one of the digits **0, 1, 2, ..., 9**, starting from an initial batch of ten random Gaussian noise images.  \n",
        "\n",
        "   Discuss the following question: do the generated images resemble real MNIST digits? Why or why not?  \n",
        "\n",
        "2. Discuss, how you would approach a second task of\n",
        "   generating an image that   \n",
        "   bares similarity to two or more digits simultaneously. **Implement your idea to see the results.**\n",
        "\n",
        "3. Third task: repeat the previous tasks with an additional L2 penalty on noise within the images. Experiment with adding `lambda_l2 * dreamed_input_batch.pow(2).mean()` loss term, with `lambda_l2` being the penalty cooefficient within an exponential progression, say from 0.001 to 10.0. Are the new digits recognized correctly? How does the penalty impact the digit quality? Explain.\n",
        "\n",
        "### **Optimization Process for Task 1**  \n",
        "\n",
        "1. Start with a **batch of 10 random Gaussian noise images** as the initial input and $(0, 1, 2, \\ldots, 9)$ as the expected output batch of target digits.  \n",
        "2. Define the objective: maximize the neural network's confidence for the corresponding target digit for each image in the batch.  \n",
        "3. Use **gradient descent** to modify the pixels in each image, making the network classify each one as the assigned digit.  \n",
        "4. Repeat until the network assigns suffieciently high confidence to each image’s target class.  \n",
        "\n",
        "### **Implementation Details**  \n",
        "\n",
        "- The neural network weights **must remain frozen** during optimization. You are modifying only the input images.  \n",
        "- The loss function should be the **cross-entropy loss** between the predicted probabilities and the desired class labels (plus an optional weighted L2 penalty regularizing the images in task 3).\n",
        "\n",
        "\n",
        "## **Points to Note**  \n",
        "\n",
        "1. **Visualize** the optimization process: Save images of the generated inputs at different steps and plot the classification confidence evolution over iterations.  \n",
        "3. **Document your findings** and explain the behavior you observe.  \n",
        "\n",
        "## **Task & Deliverables**  \n",
        "\n",
        "- A **Colab notebook** containing solutions for both tasks:\n",
        "  - The full implementation.\n",
        "  - Visualizations of the generated batch of images.\n",
        "  - A written explanation of your observations.\n",
        "- **Bonus:** If you create an **animation** showing the evolution of the input images during optimization, it will be considered a strong enhancement to your submission.\n",
        "  - You can generate an animation programmatically (e.g., using Matplotlib or OpenCV).\n",
        "  - Or, save image frames and use external tools to create a video.\n",
        "  - Provide a **link** to any video files in the README.\n",
        "- Upload your notebook and results to your **GitHub repository** for the course.\n",
        "- In the **README**, include a **link** to the notebook.\n",
        "- In the notebook, include **“Open in Colab”** badge so it can be launched directly.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 7.90MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 258kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 1.85MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 557kB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=2048,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# First copy the mnist classifier from classes:\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way,\n",
        "                                          #that output from a preceding layer serves as input to the next layer\n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.ReLU(),            #ReLU (or a Sigmoid if you want) is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on cpu\n",
            "epoch: 0 batch: 0 current batch loss: 2.3070836067199707\n",
            "epoch: 0 batch: 1 current batch loss: 2.0682969093322754\n",
            "epoch: 0 batch: 2 current batch loss: 1.5908063650131226\n",
            "epoch: 0 batch: 3 current batch loss: 1.0826318264007568\n",
            "epoch: 0 batch: 4 current batch loss: 1.0426312685012817\n",
            "epoch: 0 batch: 5 current batch loss: 1.2202101945877075\n",
            "epoch: 0 batch: 6 current batch loss: 1.2865345478057861\n",
            "epoch: 0 batch: 7 current batch loss: 0.7325815558433533\n",
            "epoch: 0 batch: 8 current batch loss: 1.0001877546310425\n",
            "epoch: 0 batch: 9 current batch loss: 0.7193752527236938\n",
            "epoch: 0 batch: 10 current batch loss: 0.6507630348205566\n",
            "epoch: 0 batch: 11 current batch loss: 0.5867834091186523\n",
            "epoch: 0 batch: 12 current batch loss: 0.5836544036865234\n",
            "epoch: 0 batch: 13 current batch loss: 0.5953906774520874\n",
            "epoch: 0 batch: 14 current batch loss: 0.54587322473526\n",
            "epoch: 0 batch: 15 current batch loss: 0.5243000388145447\n",
            "epoch: 0 batch: 16 current batch loss: 0.4840275049209595\n",
            "epoch: 0 batch: 17 current batch loss: 0.45313534140586853\n",
            "epoch: 0 batch: 18 current batch loss: 0.43052464723587036\n",
            "epoch: 0 batch: 19 current batch loss: 0.41681531071662903\n",
            "epoch: 0 batch: 20 current batch loss: 0.3969976007938385\n",
            "epoch: 0 batch: 21 current batch loss: 0.3900893032550812\n",
            "epoch: 0 batch: 22 current batch loss: 0.3605223000049591\n",
            "epoch: 0 batch: 23 current batch loss: 0.3685465455055237\n",
            "epoch: 0 batch: 24 current batch loss: 0.3382466435432434\n",
            "epoch: 0 batch: 25 current batch loss: 0.3325313329696655\n",
            "epoch: 0 batch: 26 current batch loss: 0.3263111710548401\n",
            "epoch: 0 batch: 27 current batch loss: 0.30569303035736084\n",
            "epoch: 0 batch: 28 current batch loss: 0.30776676535606384\n",
            "epoch: 0 batch: 29 current batch loss: 0.3037143647670746\n",
            "epoch: 1 batch: 0 current batch loss: 0.29453223943710327\n",
            "epoch: 1 batch: 1 current batch loss: 0.2718518376350403\n",
            "epoch: 1 batch: 2 current batch loss: 0.2760823965072632\n",
            "epoch: 1 batch: 3 current batch loss: 0.24121876060962677\n",
            "epoch: 1 batch: 4 current batch loss: 0.2635970115661621\n",
            "epoch: 1 batch: 5 current batch loss: 0.26519080996513367\n",
            "epoch: 1 batch: 6 current batch loss: 0.2671944797039032\n",
            "epoch: 1 batch: 7 current batch loss: 0.2416040301322937\n",
            "epoch: 1 batch: 8 current batch loss: 0.2845150828361511\n",
            "epoch: 1 batch: 9 current batch loss: 0.23727692663669586\n",
            "epoch: 1 batch: 10 current batch loss: 0.20862455666065216\n",
            "epoch: 1 batch: 11 current batch loss: 0.22318367660045624\n",
            "epoch: 1 batch: 12 current batch loss: 0.1905210018157959\n",
            "epoch: 1 batch: 13 current batch loss: 0.2070387303829193\n",
            "epoch: 1 batch: 14 current batch loss: 0.19211114943027496\n",
            "epoch: 1 batch: 15 current batch loss: 0.16631214320659637\n",
            "epoch: 1 batch: 16 current batch loss: 0.1994590163230896\n",
            "epoch: 1 batch: 17 current batch loss: 0.2023688405752182\n",
            "epoch: 1 batch: 18 current batch loss: 0.20677447319030762\n",
            "epoch: 1 batch: 19 current batch loss: 0.19684386253356934\n",
            "epoch: 1 batch: 20 current batch loss: 0.17524094879627228\n",
            "epoch: 1 batch: 21 current batch loss: 0.17926807701587677\n",
            "epoch: 1 batch: 22 current batch loss: 0.20631642639636993\n",
            "epoch: 1 batch: 23 current batch loss: 0.17281804978847504\n",
            "epoch: 1 batch: 24 current batch loss: 0.16614669561386108\n",
            "epoch: 1 batch: 25 current batch loss: 0.18743114173412323\n",
            "epoch: 1 batch: 26 current batch loss: 0.15821473300457\n",
            "epoch: 1 batch: 27 current batch loss: 0.18413607776165009\n",
            "epoch: 1 batch: 28 current batch loss: 0.1575496941804886\n",
            "epoch: 1 batch: 29 current batch loss: 0.1750144064426422\n",
            "epoch: 2 batch: 0 current batch loss: 0.15416838228702545\n",
            "epoch: 2 batch: 1 current batch loss: 0.14469091594219208\n",
            "epoch: 2 batch: 2 current batch loss: 0.16591304540634155\n",
            "epoch: 2 batch: 3 current batch loss: 0.14054284989833832\n",
            "epoch: 2 batch: 4 current batch loss: 0.14769084751605988\n",
            "epoch: 2 batch: 5 current batch loss: 0.1308274269104004\n",
            "epoch: 2 batch: 6 current batch loss: 0.14620482921600342\n",
            "epoch: 2 batch: 7 current batch loss: 0.1627960503101349\n",
            "epoch: 2 batch: 8 current batch loss: 0.13478407263755798\n",
            "epoch: 2 batch: 9 current batch loss: 0.12667237222194672\n",
            "epoch: 2 batch: 10 current batch loss: 0.15598556399345398\n",
            "epoch: 2 batch: 11 current batch loss: 0.12328332662582397\n",
            "epoch: 2 batch: 12 current batch loss: 0.13288667798042297\n",
            "epoch: 2 batch: 13 current batch loss: 0.12367342412471771\n",
            "epoch: 2 batch: 14 current batch loss: 0.13465195894241333\n",
            "epoch: 2 batch: 15 current batch loss: 0.13763898611068726\n",
            "epoch: 2 batch: 16 current batch loss: 0.14107537269592285\n",
            "epoch: 2 batch: 17 current batch loss: 0.12735675275325775\n",
            "epoch: 2 batch: 18 current batch loss: 0.14214228093624115\n",
            "epoch: 2 batch: 19 current batch loss: 0.1527417153120041\n",
            "epoch: 2 batch: 20 current batch loss: 0.11925538629293442\n",
            "epoch: 2 batch: 21 current batch loss: 0.15125712752342224\n",
            "epoch: 2 batch: 22 current batch loss: 0.1285647451877594\n",
            "epoch: 2 batch: 23 current batch loss: 0.1133052185177803\n",
            "epoch: 2 batch: 24 current batch loss: 0.14435575902462006\n",
            "epoch: 2 batch: 25 current batch loss: 0.13535359501838684\n",
            "epoch: 2 batch: 26 current batch loss: 0.11312959343194962\n",
            "epoch: 2 batch: 27 current batch loss: 0.11858996748924255\n",
            "epoch: 2 batch: 28 current batch loss: 0.12760548293590546\n",
            "epoch: 2 batch: 29 current batch loss: 0.12529125809669495\n",
            "epoch: 3 batch: 0 current batch loss: 0.1194692999124527\n",
            "epoch: 3 batch: 1 current batch loss: 0.11483124643564224\n",
            "epoch: 3 batch: 2 current batch loss: 0.10943039506673813\n",
            "epoch: 3 batch: 3 current batch loss: 0.10014375299215317\n",
            "epoch: 3 batch: 4 current batch loss: 0.07210547477006912\n",
            "epoch: 3 batch: 5 current batch loss: 0.1325991302728653\n",
            "epoch: 3 batch: 6 current batch loss: 0.11434365808963776\n",
            "epoch: 3 batch: 7 current batch loss: 0.11554630845785141\n",
            "epoch: 3 batch: 8 current batch loss: 0.10327863693237305\n",
            "epoch: 3 batch: 9 current batch loss: 0.10294165462255478\n",
            "epoch: 3 batch: 10 current batch loss: 0.10663776844739914\n",
            "epoch: 3 batch: 11 current batch loss: 0.10277717560529709\n",
            "epoch: 3 batch: 12 current batch loss: 0.09061332792043686\n",
            "epoch: 3 batch: 13 current batch loss: 0.09398730099201202\n",
            "epoch: 3 batch: 14 current batch loss: 0.10357848554849625\n",
            "epoch: 3 batch: 15 current batch loss: 0.10021030157804489\n",
            "epoch: 3 batch: 16 current batch loss: 0.10356336086988449\n",
            "epoch: 3 batch: 17 current batch loss: 0.08333458751440048\n",
            "epoch: 3 batch: 18 current batch loss: 0.08242641389369965\n",
            "epoch: 3 batch: 19 current batch loss: 0.08278656750917435\n",
            "epoch: 3 batch: 20 current batch loss: 0.10562314838171005\n",
            "epoch: 3 batch: 21 current batch loss: 0.10100947320461273\n",
            "epoch: 3 batch: 22 current batch loss: 0.09475888311862946\n",
            "epoch: 3 batch: 23 current batch loss: 0.09252632409334183\n",
            "epoch: 3 batch: 24 current batch loss: 0.0824553370475769\n",
            "epoch: 3 batch: 25 current batch loss: 0.0888524278998375\n",
            "epoch: 3 batch: 26 current batch loss: 0.08853434771299362\n",
            "epoch: 3 batch: 27 current batch loss: 0.09200476855039597\n",
            "epoch: 3 batch: 28 current batch loss: 0.08234785497188568\n",
            "epoch: 3 batch: 29 current batch loss: 0.09445923566818237\n",
            "epoch: 4 batch: 0 current batch loss: 0.07925744354724884\n",
            "epoch: 4 batch: 1 current batch loss: 0.0781024917960167\n",
            "epoch: 4 batch: 2 current batch loss: 0.07405262440443039\n",
            "epoch: 4 batch: 3 current batch loss: 0.06105883792042732\n",
            "epoch: 4 batch: 4 current batch loss: 0.09763634949922562\n",
            "epoch: 4 batch: 5 current batch loss: 0.06908091902732849\n",
            "epoch: 4 batch: 6 current batch loss: 0.06523267179727554\n",
            "epoch: 4 batch: 7 current batch loss: 0.08684243261814117\n",
            "epoch: 4 batch: 8 current batch loss: 0.08248217403888702\n",
            "epoch: 4 batch: 9 current batch loss: 0.07972723245620728\n",
            "epoch: 4 batch: 10 current batch loss: 0.0702706053853035\n",
            "epoch: 4 batch: 11 current batch loss: 0.0864977017045021\n",
            "epoch: 4 batch: 12 current batch loss: 0.0642191544175148\n",
            "epoch: 4 batch: 13 current batch loss: 0.07437434047460556\n",
            "epoch: 4 batch: 14 current batch loss: 0.06793907284736633\n",
            "epoch: 4 batch: 15 current batch loss: 0.0657571405172348\n",
            "epoch: 4 batch: 16 current batch loss: 0.07860512286424637\n",
            "epoch: 4 batch: 17 current batch loss: 0.07576055079698563\n",
            "epoch: 4 batch: 18 current batch loss: 0.08395933359861374\n",
            "epoch: 4 batch: 19 current batch loss: 0.07717104256153107\n",
            "epoch: 4 batch: 20 current batch loss: 0.07648638635873795\n",
            "epoch: 4 batch: 21 current batch loss: 0.06272048503160477\n",
            "epoch: 4 batch: 22 current batch loss: 0.08791162073612213\n",
            "epoch: 4 batch: 23 current batch loss: 0.08165107667446136\n",
            "epoch: 4 batch: 24 current batch loss: 0.08066179603338242\n",
            "epoch: 4 batch: 25 current batch loss: 0.07110880315303802\n",
            "epoch: 4 batch: 26 current batch loss: 0.07901257276535034\n",
            "epoch: 4 batch: 27 current batch loss: 0.07038513571023941\n",
            "epoch: 4 batch: 28 current batch loss: 0.06052021309733391\n",
            "epoch: 4 batch: 29 current batch loss: 0.07229752093553543\n",
            "epoch: 5 batch: 0 current batch loss: 0.057755500078201294\n",
            "epoch: 5 batch: 1 current batch loss: 0.06281258165836334\n",
            "epoch: 5 batch: 2 current batch loss: 0.05704066902399063\n",
            "epoch: 5 batch: 3 current batch loss: 0.060801953077316284\n",
            "epoch: 5 batch: 4 current batch loss: 0.052990954369306564\n",
            "epoch: 5 batch: 5 current batch loss: 0.05821685865521431\n",
            "epoch: 5 batch: 6 current batch loss: 0.06350447237491608\n",
            "epoch: 5 batch: 7 current batch loss: 0.07642572373151779\n",
            "epoch: 5 batch: 8 current batch loss: 0.056265320628881454\n",
            "epoch: 5 batch: 9 current batch loss: 0.05495143681764603\n",
            "epoch: 5 batch: 10 current batch loss: 0.044291187077760696\n",
            "epoch: 5 batch: 11 current batch loss: 0.0594218373298645\n",
            "epoch: 5 batch: 12 current batch loss: 0.07600340992212296\n",
            "epoch: 5 batch: 13 current batch loss: 0.05686574429273605\n",
            "epoch: 5 batch: 14 current batch loss: 0.05094245448708534\n",
            "epoch: 5 batch: 15 current batch loss: 0.05401478707790375\n",
            "epoch: 5 batch: 16 current batch loss: 0.04720504954457283\n",
            "epoch: 5 batch: 17 current batch loss: 0.06357341259717941\n",
            "epoch: 5 batch: 18 current batch loss: 0.05766425281763077\n",
            "epoch: 5 batch: 19 current batch loss: 0.0493701696395874\n",
            "epoch: 5 batch: 20 current batch loss: 0.062335893511772156\n",
            "epoch: 5 batch: 21 current batch loss: 0.0515938475728035\n",
            "epoch: 5 batch: 22 current batch loss: 0.07021339982748032\n",
            "epoch: 5 batch: 23 current batch loss: 0.06019654497504234\n",
            "epoch: 5 batch: 24 current batch loss: 0.07053691148757935\n",
            "epoch: 5 batch: 25 current batch loss: 0.06765890121459961\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(8):\n",
        "\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device) \n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)\n",
        "                                            \n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we create another model, whose job will be to\n",
        "# generate an image that MNIST classifier thinks \n",
        "# to be a given digit\n",
        "\n",
        "class Anti_MNIST(torch.nn.Module):\n",
        "    def __init__ (self, digit):\n",
        "        self.digit = digit\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
