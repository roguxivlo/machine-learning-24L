{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT2j-dwiSTmC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roguxivlo/machine-learning-24L/blob/main/hw10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment – Adversarial Examples**\n",
        "\n",
        "In this assignment, you will explore how small (invisible to humans) changes to real digits can _fool_ the CNN into misclassifying them, even though the changes are imperceptible to humans.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1 – CNN Dreams: Last Homework Assignment Revisited\n",
        "\n",
        "Re-run the input optimization process (for NN dreams) from the previous MLP-class homework assignment, but this time using the **LeNet-5 CNN model** we trained in this class.\n",
        "\n",
        "\n",
        "1. Starting from ten random noise images, optimize the input so that each image is classified with high confidence as one of the digits 0 through 9.\n",
        "2. Include an **L2 penalty** on the input to keep the images visually closer to realistic digits. Use a range of penalty strengths (e.g., $\\lambda_{l2}$ = 0, and then 0.01 through 10.0).\n",
        "3. Compare the generated images (with and without L2 penalty) to those generated by the MLP:\n",
        "   - Are they more or less readable?\n",
        "   - Do they resemble real MNIST digits more closely or less?\n",
        "   - Why do you think that happens? Consider the CNN’s inductive biases and architectural properties.\n",
        "\n",
        "Use `cross_entropy_loss + lambda_l2 * input.pow(2).mean()` as your objective.\n",
        "\n",
        "Reuse your code: visualize confidence evolution during optimization and generate image grids and (optionally) animations showing how the inputs evolve.\n",
        "\n",
        "\n",
        "## Task 2 – Adversarial Examples: Fooling LeNet-5\n",
        "\n",
        "This is the core focus of the assignment.\n",
        "\n",
        "Using a batch of **real MNIST digits** (e.g., nine examples per class), craft **adversarial examples** by adding subtle, trained noise to the input images. Your goal is to:\n",
        "\n",
        "- **Keep the human-perceived digit the same** (e.g., a \"7\" should still look like a \"7\"),\n",
        "- But **cause LeNet-5 to misclassify it** – as every other class different from the original, hence nine examples per class.\n",
        "\n",
        "### Objective\n",
        "For each image $x$ and its true label $y$, learn a perturbation $\\delta$ such that:\n",
        "\n",
        "- $\\text{LeNet5}(x + \\delta) = y_{\\text{wrong}} $,\n",
        "- and $ \\|\\delta\\|_2 $ is as small as possible (penalize large perturbations), to keep $x + \\delta$ *look* like $x$ for humans.\n",
        "\n",
        "### Optimization\n",
        "Use gradient-based optimization on $\\delta$ (the noise), while keeping the network weights frozen. Your loss might look like:\n",
        "\n",
        "```\n",
        "loss = cross_entropy(model(x + delta), target_wrong_class) +\n",
        "       lambda_l2 * delta.pow(2).mean()\n",
        "```\n",
        "\n",
        "Tune the $\\lambda_{l2}$ to find the best range.\n",
        "\n",
        "### Deliverables for the Second Task\n",
        "- Select some best examples, showing the original digit and its (correct) classification and the perturbed digit (hopefully, still looking the same to humans) and how it gets misclassified. Show them side by side.\n",
        "- Report:\n",
        "  - Success rate of attacks (it doesn't need to be very formal),\n",
        "  - Effect of $\\lambda_{l2}$ on visibility of the noise and success of misclassification,\n",
        "  - Example image grids and confidence plots.\n",
        "\n",
        "\n",
        "\n",
        "## Deliverables for the Homework Assignment\n",
        "- A Google Colab notebook with:\n",
        "  - Complete implementation for both tasks.\n",
        "  - Visualizations and animations (animations are optional but encouraged).\n",
        "  - Clear written analysis of your findings.\n",
        "- Upload the notebook and results to your GitHub repository for the course.\n",
        "- Include a link to the notebook and video (if applicable) in the `README.md`.\n",
        "- In the notebook, include “Open in Colab” badge so it can be launched directly."
      ],
      "metadata": {
        "id": "50zb5WOSep7P"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}