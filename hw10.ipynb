{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT2j-dwiSTmC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roguxivlo/machine-learning-24L/blob/main/hw10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment – Adversarial Examples**\n",
        "\n",
        "In this assignment, you will explore how small (invisible to humans) changes to real digits can _fool_ the CNN into misclassifying them, even though the changes are imperceptible to humans.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1 – CNN Dreams: Last Homework Assignment Revisited\n",
        "\n",
        "Re-run the input optimization process (for NN dreams) from the previous MLP-class homework assignment, but this time using the **LeNet-5 CNN model** we trained in this class.\n",
        "\n",
        "\n",
        "1. Starting from ten random noise images, optimize the input so that each image is classified with high confidence as one of the digits 0 through 9.\n",
        "2. Include an **L2 penalty** on the input to keep the images visually closer to realistic digits. Use a range of penalty strengths (e.g., $\\lambda_{l2}$ = 0, and then 0.01 through 10.0).\n",
        "3. Compare the generated images (with and without L2 penalty) to those generated by the MLP:\n",
        "   - Are they more or less readable?\n",
        "   - Do they resemble real MNIST digits more closely or less?\n",
        "   - Why do you think that happens? Consider the CNN’s inductive biases and architectural properties.\n",
        "\n",
        "Use `cross_entropy_loss + lambda_l2 * input.pow(2).mean()` as your objective.\n",
        "\n",
        "Reuse your code: visualize confidence evolution during optimization and generate image grids and (optionally) animations showing how the inputs evolve.\n",
        "\n",
        "\n",
        "## Task 2 – Adversarial Examples: Fooling LeNet-5\n",
        "\n",
        "This is the core focus of the assignment.\n",
        "\n",
        "Using a batch of **real MNIST digits** (e.g., nine examples per class), craft **adversarial examples** by adding subtle, trained noise to the input images. Your goal is to:\n",
        "\n",
        "- **Keep the human-perceived digit the same** (e.g., a \"7\" should still look like a \"7\"),\n",
        "- But **cause LeNet-5 to misclassify it** – as every other class different from the original, hence nine examples per class.\n",
        "\n",
        "### Objective\n",
        "For each image $x$ and its true label $y$, learn a perturbation $\\delta$ such that:\n",
        "\n",
        "- $\\text{LeNet5}(x + \\delta) = y_{\\text{wrong}} $,\n",
        "- and $ \\|\\delta\\|_2 $ is as small as possible (penalize large perturbations), to keep $x + \\delta$ *look* like $x$ for humans.\n",
        "\n",
        "### Optimization\n",
        "Use gradient-based optimization on $\\delta$ (the noise), while keeping the network weights frozen. Your loss might look like:\n",
        "\n",
        "```\n",
        "loss = cross_entropy(model(x + delta), target_wrong_class) +\n",
        "       lambda_l2 * delta.pow(2).mean()\n",
        "```\n",
        "\n",
        "Tune the $\\lambda_{l2}$ to find the best range.\n",
        "\n",
        "### Deliverables for the Second Task\n",
        "- Select some best examples, showing the original digit and its (correct) classification and the perturbed digit (hopefully, still looking the same to humans) and how it gets misclassified. Show them side by side.\n",
        "- Report:\n",
        "  - Success rate of attacks (it doesn't need to be very formal),\n",
        "  - Effect of $\\lambda_{l2}$ on visibility of the noise and success of misclassification,\n",
        "  - Example image grids and confidence plots.\n",
        "\n",
        "\n",
        "\n",
        "## Deliverables for the Homework Assignment\n",
        "- A Google Colab notebook with:\n",
        "  - Complete implementation for both tasks.\n",
        "  - Visualizations and animations (animations are optional but encouraged).\n",
        "  - Clear written analysis of your findings.\n",
        "- Upload the notebook and results to your GitHub repository for the course.\n",
        "- Include a link to the notebook and video (if applicable) in the `README.md`.\n",
        "- In the notebook, include “Open in Colab” badge so it can be launched directly."
      ],
      "metadata": {
        "id": "50zb5WOSep7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Setup"
      ],
      "metadata": {
        "id": "DB4SfzuMfKvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=None)"
      ],
      "metadata": {
        "id": "aKPzzEb7fKKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56ee04b-6c61-421b-e39b-e2af2096017a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.52MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.67MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=2048,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "id": "e6Z8h6LSfnBu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "wnt3Y-Ale9wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the LeNet5 model from our classes.\n",
        "\n",
        "# Note: I had trouble getting appropriate training convergence using sigmoid activation\n",
        "# The training works well for tanh activation function so I used this one.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet5(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5), padding=2)\n",
        "        self.pool1 = torch.nn.AvgPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5))\n",
        "        self.pool2 = torch.nn.AvgPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(5*5*16, 120)\n",
        "        self.linear2 = torch.nn.Linear(120, 84)\n",
        "        self.linear3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"Input min/max:\", x.min().item(), x.max().item()) # Before conv1\n",
        "\n",
        "        # TODO: add missing processing\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        # print(\"Input to sigmoid (after conv1) min/max:\", x.min().item(), x.max().item())\n",
        "        x = F.tanh(x)\n",
        "        # print(\"After conv1 sigmoid min/max:\", x.min().item(), x.max().item()) # Should be 0-1\n",
        "\n",
        "        x = self.pool1(x)\n",
        "        x = F.tanh(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.tanh(self.linear1(x))\n",
        "        x = F.tanh(self.linear2(x))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "oTUeqP1te6ai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training LeNet5"
      ],
      "metadata": {
        "id": "v9JAJN6igb3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = LeNet5().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(16):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the LeNet5 object. Please note,\n",
        "                                            # the nonlinear activation after the last layer is NOT applied\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBQs_-PTgaLY",
        "outputId": "2433f3c7-0f14-45d6-afa3-910c488fd1a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cuda\n",
            "epoch: 0 batch: 0 current batch loss: 2.2990927696228027\n",
            "epoch: 0 batch: 1 current batch loss: 2.256777048110962\n",
            "epoch: 0 batch: 2 current batch loss: 2.2130115032196045\n",
            "epoch: 0 batch: 3 current batch loss: 2.1664199829101562\n",
            "epoch: 0 batch: 4 current batch loss: 2.1174519062042236\n",
            "epoch: 0 batch: 5 current batch loss: 2.0560665130615234\n",
            "epoch: 0 batch: 6 current batch loss: 1.9834003448486328\n",
            "epoch: 0 batch: 7 current batch loss: 1.9292776584625244\n",
            "epoch: 0 batch: 8 current batch loss: 1.8510297536849976\n",
            "epoch: 0 batch: 9 current batch loss: 1.7657214403152466\n",
            "epoch: 0 batch: 10 current batch loss: 1.694237232208252\n",
            "epoch: 0 batch: 11 current batch loss: 1.6145609617233276\n",
            "epoch: 0 batch: 12 current batch loss: 1.5439139604568481\n",
            "epoch: 0 batch: 13 current batch loss: 1.4951133728027344\n",
            "epoch: 0 batch: 14 current batch loss: 1.4006738662719727\n",
            "epoch: 0 batch: 15 current batch loss: 1.3458646535873413\n",
            "epoch: 0 batch: 16 current batch loss: 1.2973848581314087\n",
            "epoch: 0 batch: 17 current batch loss: 1.2375444173812866\n",
            "epoch: 0 batch: 18 current batch loss: 1.1842482089996338\n",
            "epoch: 0 batch: 19 current batch loss: 1.1473660469055176\n",
            "epoch: 0 batch: 20 current batch loss: 1.0722367763519287\n",
            "epoch: 0 batch: 21 current batch loss: 1.0549097061157227\n",
            "epoch: 0 batch: 22 current batch loss: 0.9896969795227051\n",
            "epoch: 0 batch: 23 current batch loss: 0.9432207345962524\n",
            "epoch: 0 batch: 24 current batch loss: 0.9131568074226379\n",
            "epoch: 0 batch: 25 current batch loss: 0.8911368250846863\n",
            "epoch: 0 batch: 26 current batch loss: 0.8466755151748657\n",
            "epoch: 0 batch: 27 current batch loss: 0.8027329444885254\n",
            "epoch: 0 batch: 28 current batch loss: 0.784879744052887\n",
            "epoch: 0 batch: 29 current batch loss: 0.7593987584114075\n",
            "epoch: 1 batch: 0 current batch loss: 0.7352617979049683\n",
            "epoch: 1 batch: 1 current batch loss: 0.7273436784744263\n",
            "epoch: 1 batch: 2 current batch loss: 0.6813061237335205\n",
            "epoch: 1 batch: 3 current batch loss: 0.671191394329071\n",
            "epoch: 1 batch: 4 current batch loss: 0.656440258026123\n",
            "epoch: 1 batch: 5 current batch loss: 0.6051827669143677\n",
            "epoch: 1 batch: 6 current batch loss: 0.6285486221313477\n",
            "epoch: 1 batch: 7 current batch loss: 0.5809054374694824\n",
            "epoch: 1 batch: 8 current batch loss: 0.5916163325309753\n",
            "epoch: 1 batch: 9 current batch loss: 0.5737919211387634\n",
            "epoch: 1 batch: 10 current batch loss: 0.5678752660751343\n",
            "epoch: 1 batch: 11 current batch loss: 0.5179358124732971\n",
            "epoch: 1 batch: 12 current batch loss: 0.5139177441596985\n",
            "epoch: 1 batch: 13 current batch loss: 0.5244243144989014\n",
            "epoch: 1 batch: 14 current batch loss: 0.4914088547229767\n",
            "epoch: 1 batch: 15 current batch loss: 0.47464895248413086\n",
            "epoch: 1 batch: 16 current batch loss: 0.4479018747806549\n",
            "epoch: 1 batch: 17 current batch loss: 0.49450716376304626\n",
            "epoch: 1 batch: 18 current batch loss: 0.4498603940010071\n",
            "epoch: 1 batch: 19 current batch loss: 0.4436281621456146\n",
            "epoch: 1 batch: 20 current batch loss: 0.4403216242790222\n",
            "epoch: 1 batch: 21 current batch loss: 0.4349908232688904\n",
            "epoch: 1 batch: 22 current batch loss: 0.4058178961277008\n",
            "epoch: 1 batch: 23 current batch loss: 0.4030897617340088\n",
            "epoch: 1 batch: 24 current batch loss: 0.41107773780822754\n",
            "epoch: 1 batch: 25 current batch loss: 0.42698201537132263\n",
            "epoch: 1 batch: 26 current batch loss: 0.4104542136192322\n",
            "epoch: 1 batch: 27 current batch loss: 0.41742634773254395\n",
            "epoch: 1 batch: 28 current batch loss: 0.39320191740989685\n",
            "epoch: 1 batch: 29 current batch loss: 0.3721409738063812\n",
            "epoch: 2 batch: 0 current batch loss: 0.37666213512420654\n",
            "epoch: 2 batch: 1 current batch loss: 0.376125305891037\n",
            "epoch: 2 batch: 2 current batch loss: 0.35676151514053345\n",
            "epoch: 2 batch: 3 current batch loss: 0.3612990379333496\n",
            "epoch: 2 batch: 4 current batch loss: 0.37157922983169556\n",
            "epoch: 2 batch: 5 current batch loss: 0.34387150406837463\n",
            "epoch: 2 batch: 6 current batch loss: 0.3448985815048218\n",
            "epoch: 2 batch: 7 current batch loss: 0.37635448575019836\n",
            "epoch: 2 batch: 8 current batch loss: 0.334686279296875\n",
            "epoch: 2 batch: 9 current batch loss: 0.35278311371803284\n",
            "epoch: 2 batch: 10 current batch loss: 0.3047868013381958\n",
            "epoch: 2 batch: 11 current batch loss: 0.31309252977371216\n",
            "epoch: 2 batch: 12 current batch loss: 0.32557326555252075\n",
            "epoch: 2 batch: 13 current batch loss: 0.33466702699661255\n",
            "epoch: 2 batch: 14 current batch loss: 0.36425459384918213\n",
            "epoch: 2 batch: 15 current batch loss: 0.30886033177375793\n",
            "epoch: 2 batch: 16 current batch loss: 0.2886381149291992\n",
            "epoch: 2 batch: 17 current batch loss: 0.3116550147533417\n",
            "epoch: 2 batch: 18 current batch loss: 0.28276100754737854\n",
            "epoch: 2 batch: 19 current batch loss: 0.29996398091316223\n",
            "epoch: 2 batch: 20 current batch loss: 0.2805730104446411\n",
            "epoch: 2 batch: 21 current batch loss: 0.3190481960773468\n",
            "epoch: 2 batch: 22 current batch loss: 0.29761579632759094\n",
            "epoch: 2 batch: 23 current batch loss: 0.2849055528640747\n",
            "epoch: 2 batch: 24 current batch loss: 0.3149542212486267\n",
            "epoch: 2 batch: 25 current batch loss: 0.309585303068161\n",
            "epoch: 2 batch: 26 current batch loss: 0.2915772795677185\n",
            "epoch: 2 batch: 27 current batch loss: 0.2971958816051483\n",
            "epoch: 2 batch: 28 current batch loss: 0.3122027814388275\n",
            "epoch: 2 batch: 29 current batch loss: 0.2662062346935272\n",
            "epoch: 3 batch: 0 current batch loss: 0.2501259446144104\n",
            "epoch: 3 batch: 1 current batch loss: 0.2774215042591095\n",
            "epoch: 3 batch: 2 current batch loss: 0.2851024270057678\n",
            "epoch: 3 batch: 3 current batch loss: 0.23577915132045746\n",
            "epoch: 3 batch: 4 current batch loss: 0.2456340342760086\n",
            "epoch: 3 batch: 5 current batch loss: 0.25726431608200073\n",
            "epoch: 3 batch: 6 current batch loss: 0.2553008198738098\n",
            "epoch: 3 batch: 7 current batch loss: 0.24770432710647583\n",
            "epoch: 3 batch: 8 current batch loss: 0.26709601283073425\n",
            "epoch: 3 batch: 9 current batch loss: 0.2690640091896057\n",
            "epoch: 3 batch: 10 current batch loss: 0.23940399289131165\n",
            "epoch: 3 batch: 11 current batch loss: 0.27251896262168884\n",
            "epoch: 3 batch: 12 current batch loss: 0.23555903136730194\n",
            "epoch: 3 batch: 13 current batch loss: 0.24372224509716034\n",
            "epoch: 3 batch: 14 current batch loss: 0.271677166223526\n",
            "epoch: 3 batch: 15 current batch loss: 0.25881683826446533\n",
            "epoch: 3 batch: 16 current batch loss: 0.22296775877475739\n",
            "epoch: 3 batch: 17 current batch loss: 0.20958207547664642\n",
            "epoch: 3 batch: 18 current batch loss: 0.24307018518447876\n",
            "epoch: 3 batch: 19 current batch loss: 0.2665926516056061\n",
            "epoch: 3 batch: 20 current batch loss: 0.23582327365875244\n",
            "epoch: 3 batch: 21 current batch loss: 0.23122000694274902\n",
            "epoch: 3 batch: 22 current batch loss: 0.24155347049236298\n",
            "epoch: 3 batch: 23 current batch loss: 0.2173820585012436\n",
            "epoch: 3 batch: 24 current batch loss: 0.21483708918094635\n",
            "epoch: 3 batch: 25 current batch loss: 0.2307063192129135\n",
            "epoch: 3 batch: 26 current batch loss: 0.22921356558799744\n",
            "epoch: 3 batch: 27 current batch loss: 0.21363431215286255\n",
            "epoch: 3 batch: 28 current batch loss: 0.21317774057388306\n",
            "epoch: 3 batch: 29 current batch loss: 0.24781382083892822\n",
            "epoch: 4 batch: 0 current batch loss: 0.2081337869167328\n",
            "epoch: 4 batch: 1 current batch loss: 0.22147169709205627\n",
            "epoch: 4 batch: 2 current batch loss: 0.22818535566329956\n",
            "epoch: 4 batch: 3 current batch loss: 0.2189946323633194\n",
            "epoch: 4 batch: 4 current batch loss: 0.19837966561317444\n",
            "epoch: 4 batch: 5 current batch loss: 0.19931678473949432\n",
            "epoch: 4 batch: 6 current batch loss: 0.20089532434940338\n",
            "epoch: 4 batch: 7 current batch loss: 0.20018360018730164\n",
            "epoch: 4 batch: 8 current batch loss: 0.19700710475444794\n",
            "epoch: 4 batch: 9 current batch loss: 0.19877931475639343\n",
            "epoch: 4 batch: 10 current batch loss: 0.21760205924510956\n",
            "epoch: 4 batch: 11 current batch loss: 0.19600045680999756\n",
            "epoch: 4 batch: 12 current batch loss: 0.1813463270664215\n",
            "epoch: 4 batch: 13 current batch loss: 0.19499488174915314\n",
            "epoch: 4 batch: 14 current batch loss: 0.16352300345897675\n",
            "epoch: 4 batch: 15 current batch loss: 0.18678432703018188\n",
            "epoch: 4 batch: 16 current batch loss: 0.1729411482810974\n",
            "epoch: 4 batch: 17 current batch loss: 0.19353264570236206\n",
            "epoch: 4 batch: 18 current batch loss: 0.17492330074310303\n",
            "epoch: 4 batch: 19 current batch loss: 0.18487398326396942\n",
            "epoch: 4 batch: 20 current batch loss: 0.14778155088424683\n",
            "epoch: 4 batch: 21 current batch loss: 0.1829906851053238\n",
            "epoch: 4 batch: 22 current batch loss: 0.18296074867248535\n",
            "epoch: 4 batch: 23 current batch loss: 0.1776845008134842\n",
            "epoch: 4 batch: 24 current batch loss: 0.17593780159950256\n",
            "epoch: 4 batch: 25 current batch loss: 0.1855112910270691\n",
            "epoch: 4 batch: 26 current batch loss: 0.17133548855781555\n",
            "epoch: 4 batch: 27 current batch loss: 0.1649087816476822\n",
            "epoch: 4 batch: 28 current batch loss: 0.15438151359558105\n",
            "epoch: 4 batch: 29 current batch loss: 0.15409374237060547\n",
            "epoch: 5 batch: 0 current batch loss: 0.15883967280387878\n",
            "epoch: 5 batch: 1 current batch loss: 0.16026197373867035\n",
            "epoch: 5 batch: 2 current batch loss: 0.15368036925792694\n",
            "epoch: 5 batch: 3 current batch loss: 0.16854892671108246\n",
            "epoch: 5 batch: 4 current batch loss: 0.1716081202030182\n",
            "epoch: 5 batch: 5 current batch loss: 0.16613392531871796\n",
            "epoch: 5 batch: 6 current batch loss: 0.1416410654783249\n",
            "epoch: 5 batch: 7 current batch loss: 0.15968823432922363\n",
            "epoch: 5 batch: 8 current batch loss: 0.1386210322380066\n",
            "epoch: 5 batch: 9 current batch loss: 0.14453138411045074\n",
            "epoch: 5 batch: 10 current batch loss: 0.1358424574136734\n",
            "epoch: 5 batch: 11 current batch loss: 0.16336758434772491\n",
            "epoch: 5 batch: 12 current batch loss: 0.1647699773311615\n",
            "epoch: 5 batch: 13 current batch loss: 0.13136538863182068\n",
            "epoch: 5 batch: 14 current batch loss: 0.15215528011322021\n",
            "epoch: 5 batch: 15 current batch loss: 0.15787453949451447\n",
            "epoch: 5 batch: 16 current batch loss: 0.15809549391269684\n",
            "epoch: 5 batch: 17 current batch loss: 0.156991645693779\n",
            "epoch: 5 batch: 18 current batch loss: 0.14757627248764038\n",
            "epoch: 5 batch: 19 current batch loss: 0.1356969028711319\n",
            "epoch: 5 batch: 20 current batch loss: 0.11994008719921112\n",
            "epoch: 5 batch: 21 current batch loss: 0.15639954805374146\n",
            "epoch: 5 batch: 22 current batch loss: 0.13172109425067902\n",
            "epoch: 5 batch: 23 current batch loss: 0.14753161370754242\n",
            "epoch: 5 batch: 24 current batch loss: 0.13131313025951385\n",
            "epoch: 5 batch: 25 current batch loss: 0.14546777307987213\n",
            "epoch: 5 batch: 26 current batch loss: 0.12934443354606628\n",
            "epoch: 5 batch: 27 current batch loss: 0.12111033499240875\n",
            "epoch: 5 batch: 28 current batch loss: 0.12794244289398193\n",
            "epoch: 5 batch: 29 current batch loss: 0.1289842575788498\n",
            "epoch: 6 batch: 0 current batch loss: 0.14053575694561005\n",
            "epoch: 6 batch: 1 current batch loss: 0.13369585573673248\n",
            "epoch: 6 batch: 2 current batch loss: 0.13115820288658142\n",
            "epoch: 6 batch: 3 current batch loss: 0.11228659003973007\n",
            "epoch: 6 batch: 4 current batch loss: 0.12444403767585754\n",
            "epoch: 6 batch: 5 current batch loss: 0.13951514661312103\n",
            "epoch: 6 batch: 6 current batch loss: 0.11500494182109833\n",
            "epoch: 6 batch: 7 current batch loss: 0.1215190663933754\n",
            "epoch: 6 batch: 8 current batch loss: 0.12024835497140884\n",
            "epoch: 6 batch: 9 current batch loss: 0.11816128343343735\n",
            "epoch: 6 batch: 10 current batch loss: 0.11448778212070465\n",
            "epoch: 6 batch: 11 current batch loss: 0.11988870799541473\n",
            "epoch: 6 batch: 12 current batch loss: 0.12128652632236481\n",
            "epoch: 6 batch: 13 current batch loss: 0.10966698080301285\n",
            "epoch: 6 batch: 14 current batch loss: 0.1131083145737648\n",
            "epoch: 6 batch: 15 current batch loss: 0.12210201472043991\n",
            "epoch: 6 batch: 16 current batch loss: 0.11062343418598175\n",
            "epoch: 6 batch: 17 current batch loss: 0.11855272948741913\n",
            "epoch: 6 batch: 18 current batch loss: 0.11250808089971542\n",
            "epoch: 6 batch: 19 current batch loss: 0.09982790052890778\n",
            "epoch: 6 batch: 20 current batch loss: 0.11525760591030121\n",
            "epoch: 6 batch: 21 current batch loss: 0.11104004085063934\n",
            "epoch: 6 batch: 22 current batch loss: 0.11774991452693939\n",
            "epoch: 6 batch: 23 current batch loss: 0.10575276613235474\n",
            "epoch: 6 batch: 24 current batch loss: 0.11211053282022476\n",
            "epoch: 6 batch: 25 current batch loss: 0.11605461686849594\n",
            "epoch: 6 batch: 26 current batch loss: 0.129390686750412\n",
            "epoch: 6 batch: 27 current batch loss: 0.11167331039905548\n",
            "epoch: 6 batch: 28 current batch loss: 0.09927487373352051\n",
            "epoch: 6 batch: 29 current batch loss: 0.1175946593284607\n",
            "epoch: 7 batch: 0 current batch loss: 0.0976257398724556\n",
            "epoch: 7 batch: 1 current batch loss: 0.10385214537382126\n",
            "epoch: 7 batch: 2 current batch loss: 0.09717274457216263\n",
            "epoch: 7 batch: 3 current batch loss: 0.08941935002803802\n",
            "epoch: 7 batch: 4 current batch loss: 0.09096518158912659\n",
            "epoch: 7 batch: 5 current batch loss: 0.0913759395480156\n",
            "epoch: 7 batch: 6 current batch loss: 0.10005282610654831\n",
            "epoch: 7 batch: 7 current batch loss: 0.10908228904008865\n",
            "epoch: 7 batch: 8 current batch loss: 0.10594149678945541\n",
            "epoch: 7 batch: 9 current batch loss: 0.10939060151576996\n",
            "epoch: 7 batch: 10 current batch loss: 0.09527983516454697\n",
            "epoch: 7 batch: 11 current batch loss: 0.10814645886421204\n",
            "epoch: 7 batch: 12 current batch loss: 0.10304372757673264\n",
            "epoch: 7 batch: 13 current batch loss: 0.09082837402820587\n",
            "epoch: 7 batch: 14 current batch loss: 0.09600119292736053\n",
            "epoch: 7 batch: 15 current batch loss: 0.10569513589143753\n",
            "epoch: 7 batch: 16 current batch loss: 0.10979585349559784\n",
            "epoch: 7 batch: 17 current batch loss: 0.09589157998561859\n",
            "epoch: 7 batch: 18 current batch loss: 0.09251123666763306\n",
            "epoch: 7 batch: 19 current batch loss: 0.10782549530267715\n",
            "epoch: 7 batch: 20 current batch loss: 0.10653449594974518\n",
            "epoch: 7 batch: 21 current batch loss: 0.09823022782802582\n",
            "epoch: 7 batch: 22 current batch loss: 0.08458229154348373\n",
            "epoch: 7 batch: 23 current batch loss: 0.10228405147790909\n",
            "epoch: 7 batch: 24 current batch loss: 0.09328465163707733\n",
            "epoch: 7 batch: 25 current batch loss: 0.10052292048931122\n",
            "epoch: 7 batch: 26 current batch loss: 0.08444985002279282\n",
            "epoch: 7 batch: 27 current batch loss: 0.09608270227909088\n",
            "epoch: 7 batch: 28 current batch loss: 0.09121188521385193\n",
            "epoch: 7 batch: 29 current batch loss: 0.0777852013707161\n",
            "epoch: 8 batch: 0 current batch loss: 0.0861792266368866\n",
            "epoch: 8 batch: 1 current batch loss: 0.09505170583724976\n",
            "epoch: 8 batch: 2 current batch loss: 0.08056721836328506\n",
            "epoch: 8 batch: 3 current batch loss: 0.09446809440851212\n",
            "epoch: 8 batch: 4 current batch loss: 0.07937599718570709\n",
            "epoch: 8 batch: 5 current batch loss: 0.09148937463760376\n",
            "epoch: 8 batch: 6 current batch loss: 0.06625846028327942\n",
            "epoch: 8 batch: 7 current batch loss: 0.08231264352798462\n",
            "epoch: 8 batch: 8 current batch loss: 0.10908754169940948\n",
            "epoch: 8 batch: 9 current batch loss: 0.0742279663681984\n",
            "epoch: 8 batch: 10 current batch loss: 0.09084975719451904\n",
            "epoch: 8 batch: 11 current batch loss: 0.07872448861598969\n",
            "epoch: 8 batch: 12 current batch loss: 0.08442381024360657\n",
            "epoch: 8 batch: 13 current batch loss: 0.07098915427923203\n",
            "epoch: 8 batch: 14 current batch loss: 0.06291303038597107\n",
            "epoch: 8 batch: 15 current batch loss: 0.08027154952287674\n",
            "epoch: 8 batch: 16 current batch loss: 0.08148596435785294\n",
            "epoch: 8 batch: 17 current batch loss: 0.08821696043014526\n",
            "epoch: 8 batch: 18 current batch loss: 0.07533764094114304\n",
            "epoch: 8 batch: 19 current batch loss: 0.07956543564796448\n",
            "epoch: 8 batch: 20 current batch loss: 0.08614125102758408\n",
            "epoch: 8 batch: 21 current batch loss: 0.08688291162252426\n",
            "epoch: 8 batch: 22 current batch loss: 0.09543205052614212\n",
            "epoch: 8 batch: 23 current batch loss: 0.0787307396531105\n",
            "epoch: 8 batch: 24 current batch loss: 0.07688887417316437\n",
            "epoch: 8 batch: 25 current batch loss: 0.06756961345672607\n",
            "epoch: 8 batch: 26 current batch loss: 0.09318423271179199\n",
            "epoch: 8 batch: 27 current batch loss: 0.08528167009353638\n",
            "epoch: 8 batch: 28 current batch loss: 0.07555274665355682\n",
            "epoch: 8 batch: 29 current batch loss: 0.0809345468878746\n",
            "epoch: 9 batch: 0 current batch loss: 0.07281129062175751\n",
            "epoch: 9 batch: 1 current batch loss: 0.07161801308393478\n",
            "epoch: 9 batch: 2 current batch loss: 0.08322001993656158\n",
            "epoch: 9 batch: 3 current batch loss: 0.06345368921756744\n",
            "epoch: 9 batch: 4 current batch loss: 0.06684348732233047\n",
            "epoch: 9 batch: 5 current batch loss: 0.06856241077184677\n",
            "epoch: 9 batch: 6 current batch loss: 0.07554709911346436\n",
            "epoch: 9 batch: 7 current batch loss: 0.0650201067328453\n",
            "epoch: 9 batch: 8 current batch loss: 0.07149682939052582\n",
            "epoch: 9 batch: 9 current batch loss: 0.07134221494197845\n",
            "epoch: 9 batch: 10 current batch loss: 0.0872391015291214\n",
            "epoch: 9 batch: 11 current batch loss: 0.07576187700033188\n",
            "epoch: 9 batch: 12 current batch loss: 0.06536311656236649\n",
            "epoch: 9 batch: 13 current batch loss: 0.06746216863393784\n",
            "epoch: 9 batch: 14 current batch loss: 0.0698736235499382\n",
            "epoch: 9 batch: 15 current batch loss: 0.06009044498205185\n",
            "epoch: 9 batch: 16 current batch loss: 0.06728024780750275\n",
            "epoch: 9 batch: 17 current batch loss: 0.07403555512428284\n",
            "epoch: 9 batch: 18 current batch loss: 0.08434644341468811\n",
            "epoch: 9 batch: 19 current batch loss: 0.06580765545368195\n",
            "epoch: 9 batch: 20 current batch loss: 0.06307949125766754\n",
            "epoch: 9 batch: 21 current batch loss: 0.07752993702888489\n",
            "epoch: 9 batch: 22 current batch loss: 0.06930885463953018\n",
            "epoch: 9 batch: 23 current batch loss: 0.06366755068302155\n",
            "epoch: 9 batch: 24 current batch loss: 0.06994087249040604\n",
            "epoch: 9 batch: 25 current batch loss: 0.08325433731079102\n",
            "epoch: 9 batch: 26 current batch loss: 0.08139824122190475\n",
            "epoch: 9 batch: 27 current batch loss: 0.06786171346902847\n",
            "epoch: 9 batch: 28 current batch loss: 0.08267558366060257\n",
            "epoch: 9 batch: 29 current batch loss: 0.09851744771003723\n",
            "epoch: 10 batch: 0 current batch loss: 0.07862377166748047\n",
            "epoch: 10 batch: 1 current batch loss: 0.053792692720890045\n",
            "epoch: 10 batch: 2 current batch loss: 0.06745762377977371\n",
            "epoch: 10 batch: 3 current batch loss: 0.05283505469560623\n",
            "epoch: 10 batch: 4 current batch loss: 0.07316208630800247\n",
            "epoch: 10 batch: 5 current batch loss: 0.06319086253643036\n",
            "epoch: 10 batch: 6 current batch loss: 0.0636102706193924\n",
            "epoch: 10 batch: 7 current batch loss: 0.06411287188529968\n",
            "epoch: 10 batch: 8 current batch loss: 0.07221551239490509\n",
            "epoch: 10 batch: 9 current batch loss: 0.06822721660137177\n",
            "epoch: 10 batch: 10 current batch loss: 0.07841019332408905\n",
            "epoch: 10 batch: 11 current batch loss: 0.06849998235702515\n",
            "epoch: 10 batch: 12 current batch loss: 0.0680214911699295\n",
            "epoch: 10 batch: 13 current batch loss: 0.06292930245399475\n",
            "epoch: 10 batch: 14 current batch loss: 0.07013024389743805\n",
            "epoch: 10 batch: 15 current batch loss: 0.06757117807865143\n",
            "epoch: 10 batch: 16 current batch loss: 0.052375197410583496\n",
            "epoch: 10 batch: 17 current batch loss: 0.07656746357679367\n",
            "epoch: 10 batch: 18 current batch loss: 0.0533323772251606\n",
            "epoch: 10 batch: 19 current batch loss: 0.06898675113916397\n",
            "epoch: 10 batch: 20 current batch loss: 0.05198442190885544\n",
            "epoch: 10 batch: 21 current batch loss: 0.056628432124853134\n",
            "epoch: 10 batch: 22 current batch loss: 0.06018301472067833\n",
            "epoch: 10 batch: 23 current batch loss: 0.0716392993927002\n",
            "epoch: 10 batch: 24 current batch loss: 0.05378640815615654\n",
            "epoch: 10 batch: 25 current batch loss: 0.05262390524148941\n",
            "epoch: 10 batch: 26 current batch loss: 0.059114016592502594\n",
            "epoch: 10 batch: 27 current batch loss: 0.06697160005569458\n",
            "epoch: 10 batch: 28 current batch loss: 0.054929304867982864\n",
            "epoch: 10 batch: 29 current batch loss: 0.062474142760038376\n",
            "epoch: 11 batch: 0 current batch loss: 0.06419089436531067\n",
            "epoch: 11 batch: 1 current batch loss: 0.05780553072690964\n",
            "epoch: 11 batch: 2 current batch loss: 0.06214187666773796\n",
            "epoch: 11 batch: 3 current batch loss: 0.06703094393014908\n",
            "epoch: 11 batch: 4 current batch loss: 0.059754349291324615\n",
            "epoch: 11 batch: 5 current batch loss: 0.06123441457748413\n",
            "epoch: 11 batch: 6 current batch loss: 0.06761981546878815\n",
            "epoch: 11 batch: 7 current batch loss: 0.04809539020061493\n",
            "epoch: 11 batch: 8 current batch loss: 0.06512615084648132\n",
            "epoch: 11 batch: 9 current batch loss: 0.05737487971782684\n",
            "epoch: 11 batch: 10 current batch loss: 0.0633377805352211\n",
            "epoch: 11 batch: 11 current batch loss: 0.045425575226545334\n",
            "epoch: 11 batch: 12 current batch loss: 0.06892739236354828\n",
            "epoch: 11 batch: 13 current batch loss: 0.0692456066608429\n",
            "epoch: 11 batch: 14 current batch loss: 0.05275259539484978\n",
            "epoch: 11 batch: 15 current batch loss: 0.0527491495013237\n",
            "epoch: 11 batch: 16 current batch loss: 0.048937033861875534\n",
            "epoch: 11 batch: 17 current batch loss: 0.05026722699403763\n",
            "epoch: 11 batch: 18 current batch loss: 0.05006897822022438\n",
            "epoch: 11 batch: 19 current batch loss: 0.04588194563984871\n",
            "epoch: 11 batch: 20 current batch loss: 0.058555614203214645\n",
            "epoch: 11 batch: 21 current batch loss: 0.0565105676651001\n",
            "epoch: 11 batch: 22 current batch loss: 0.0475592315196991\n",
            "epoch: 11 batch: 23 current batch loss: 0.06456180661916733\n",
            "epoch: 11 batch: 24 current batch loss: 0.05878586322069168\n",
            "epoch: 11 batch: 25 current batch loss: 0.05394667014479637\n",
            "epoch: 11 batch: 26 current batch loss: 0.07107953727245331\n",
            "epoch: 11 batch: 27 current batch loss: 0.057478778064250946\n",
            "epoch: 11 batch: 28 current batch loss: 0.050609856843948364\n",
            "epoch: 11 batch: 29 current batch loss: 0.05272065848112106\n",
            "epoch: 12 batch: 0 current batch loss: 0.048737771809101105\n",
            "epoch: 12 batch: 1 current batch loss: 0.04966110736131668\n",
            "epoch: 12 batch: 2 current batch loss: 0.058599404990673065\n",
            "epoch: 12 batch: 3 current batch loss: 0.05141414329409599\n",
            "epoch: 12 batch: 4 current batch loss: 0.05057369917631149\n",
            "epoch: 12 batch: 5 current batch loss: 0.06913664191961288\n",
            "epoch: 12 batch: 6 current batch loss: 0.05908168479800224\n",
            "epoch: 12 batch: 7 current batch loss: 0.05723254010081291\n",
            "epoch: 12 batch: 8 current batch loss: 0.05280443653464317\n",
            "epoch: 12 batch: 9 current batch loss: 0.04942333698272705\n",
            "epoch: 12 batch: 10 current batch loss: 0.05971367284655571\n",
            "epoch: 12 batch: 11 current batch loss: 0.0559283010661602\n",
            "epoch: 12 batch: 12 current batch loss: 0.045951321721076965\n",
            "epoch: 12 batch: 13 current batch loss: 0.05199790373444557\n",
            "epoch: 12 batch: 14 current batch loss: 0.048418473452329636\n",
            "epoch: 12 batch: 15 current batch loss: 0.05104590952396393\n",
            "epoch: 12 batch: 16 current batch loss: 0.05550061911344528\n",
            "epoch: 12 batch: 17 current batch loss: 0.056656721979379654\n",
            "epoch: 12 batch: 18 current batch loss: 0.0467439629137516\n",
            "epoch: 12 batch: 19 current batch loss: 0.05434763804078102\n",
            "epoch: 12 batch: 20 current batch loss: 0.04896387830376625\n",
            "epoch: 12 batch: 21 current batch loss: 0.05429857224225998\n",
            "epoch: 12 batch: 22 current batch loss: 0.04206841439008713\n",
            "epoch: 12 batch: 23 current batch loss: 0.05476146563887596\n",
            "epoch: 12 batch: 24 current batch loss: 0.04025109112262726\n",
            "epoch: 12 batch: 25 current batch loss: 0.04444535821676254\n",
            "epoch: 12 batch: 26 current batch loss: 0.04645919427275658\n",
            "epoch: 12 batch: 27 current batch loss: 0.055641986429691315\n",
            "epoch: 12 batch: 28 current batch loss: 0.045031193643808365\n",
            "epoch: 12 batch: 29 current batch loss: 0.042792778462171555\n",
            "epoch: 13 batch: 0 current batch loss: 0.048548996448516846\n",
            "epoch: 13 batch: 1 current batch loss: 0.046065788716077805\n",
            "epoch: 13 batch: 2 current batch loss: 0.039432343095541\n",
            "epoch: 13 batch: 3 current batch loss: 0.04597609117627144\n",
            "epoch: 13 batch: 4 current batch loss: 0.036802493035793304\n",
            "epoch: 13 batch: 5 current batch loss: 0.06748040020465851\n",
            "epoch: 13 batch: 6 current batch loss: 0.04509729892015457\n",
            "epoch: 13 batch: 7 current batch loss: 0.05252470821142197\n",
            "epoch: 13 batch: 8 current batch loss: 0.051483213901519775\n",
            "epoch: 13 batch: 9 current batch loss: 0.042947474867105484\n",
            "epoch: 13 batch: 10 current batch loss: 0.05091887712478638\n",
            "epoch: 13 batch: 11 current batch loss: 0.047400087118148804\n",
            "epoch: 13 batch: 12 current batch loss: 0.039771270006895065\n",
            "epoch: 13 batch: 13 current batch loss: 0.05900763347744942\n",
            "epoch: 13 batch: 14 current batch loss: 0.041476741433143616\n",
            "epoch: 13 batch: 15 current batch loss: 0.048711758106946945\n",
            "epoch: 13 batch: 16 current batch loss: 0.0493595227599144\n",
            "epoch: 13 batch: 17 current batch loss: 0.04501689225435257\n",
            "epoch: 13 batch: 18 current batch loss: 0.05042815953493118\n",
            "epoch: 13 batch: 19 current batch loss: 0.04530636593699455\n",
            "epoch: 13 batch: 20 current batch loss: 0.049363039433956146\n",
            "epoch: 13 batch: 21 current batch loss: 0.05058310553431511\n",
            "epoch: 13 batch: 22 current batch loss: 0.03937642276287079\n",
            "epoch: 13 batch: 23 current batch loss: 0.04986885190010071\n",
            "epoch: 13 batch: 24 current batch loss: 0.050205573439598083\n",
            "epoch: 13 batch: 25 current batch loss: 0.045105017721652985\n",
            "epoch: 13 batch: 26 current batch loss: 0.0560237392783165\n",
            "epoch: 13 batch: 27 current batch loss: 0.05511952191591263\n",
            "epoch: 13 batch: 28 current batch loss: 0.03420374169945717\n",
            "epoch: 13 batch: 29 current batch loss: 0.05412472411990166\n",
            "epoch: 14 batch: 0 current batch loss: 0.03835947439074516\n",
            "epoch: 14 batch: 1 current batch loss: 0.06084858998656273\n",
            "epoch: 14 batch: 2 current batch loss: 0.03634835407137871\n",
            "epoch: 14 batch: 3 current batch loss: 0.05304586514830589\n",
            "epoch: 14 batch: 4 current batch loss: 0.04148271307349205\n",
            "epoch: 14 batch: 5 current batch loss: 0.052585165947675705\n",
            "epoch: 14 batch: 6 current batch loss: 0.046646397560834885\n",
            "epoch: 14 batch: 7 current batch loss: 0.04692968726158142\n",
            "epoch: 14 batch: 8 current batch loss: 0.03419581428170204\n",
            "epoch: 14 batch: 9 current batch loss: 0.03743722289800644\n",
            "epoch: 14 batch: 10 current batch loss: 0.044402677565813065\n",
            "epoch: 14 batch: 11 current batch loss: 0.03716403618454933\n",
            "epoch: 14 batch: 12 current batch loss: 0.047540269792079926\n",
            "epoch: 14 batch: 13 current batch loss: 0.04370558261871338\n",
            "epoch: 14 batch: 14 current batch loss: 0.04117567464709282\n",
            "epoch: 14 batch: 15 current batch loss: 0.04119010269641876\n",
            "epoch: 14 batch: 16 current batch loss: 0.0502183772623539\n",
            "epoch: 14 batch: 17 current batch loss: 0.043892212212085724\n",
            "epoch: 14 batch: 18 current batch loss: 0.050082478672266006\n",
            "epoch: 14 batch: 19 current batch loss: 0.0486149825155735\n",
            "epoch: 14 batch: 20 current batch loss: 0.046288613229990005\n",
            "epoch: 14 batch: 21 current batch loss: 0.04085955023765564\n",
            "epoch: 14 batch: 22 current batch loss: 0.049333903938531876\n",
            "epoch: 14 batch: 23 current batch loss: 0.03509238734841347\n",
            "epoch: 14 batch: 24 current batch loss: 0.0315127931535244\n",
            "epoch: 14 batch: 25 current batch loss: 0.04742269590497017\n",
            "epoch: 14 batch: 26 current batch loss: 0.049039971083402634\n",
            "epoch: 14 batch: 27 current batch loss: 0.052226722240448\n",
            "epoch: 14 batch: 28 current batch loss: 0.037703655660152435\n",
            "epoch: 14 batch: 29 current batch loss: 0.038499899208545685\n",
            "epoch: 15 batch: 0 current batch loss: 0.04387972503900528\n",
            "epoch: 15 batch: 1 current batch loss: 0.046278104186058044\n",
            "epoch: 15 batch: 2 current batch loss: 0.039124201983213425\n",
            "epoch: 15 batch: 3 current batch loss: 0.0485088974237442\n",
            "epoch: 15 batch: 4 current batch loss: 0.03536715358495712\n",
            "epoch: 15 batch: 5 current batch loss: 0.048266127705574036\n",
            "epoch: 15 batch: 6 current batch loss: 0.04544384032487869\n",
            "epoch: 15 batch: 7 current batch loss: 0.031632110476493835\n",
            "epoch: 15 batch: 8 current batch loss: 0.03562582656741142\n",
            "epoch: 15 batch: 9 current batch loss: 0.035440415143966675\n",
            "epoch: 15 batch: 10 current batch loss: 0.04431719705462456\n",
            "epoch: 15 batch: 11 current batch loss: 0.04398149996995926\n",
            "epoch: 15 batch: 12 current batch loss: 0.039487872272729874\n",
            "epoch: 15 batch: 13 current batch loss: 0.04223956912755966\n",
            "epoch: 15 batch: 14 current batch loss: 0.042683735489845276\n",
            "epoch: 15 batch: 15 current batch loss: 0.039086807519197464\n",
            "epoch: 15 batch: 16 current batch loss: 0.036833517253398895\n",
            "epoch: 15 batch: 17 current batch loss: 0.04354020953178406\n",
            "epoch: 15 batch: 18 current batch loss: 0.047446247190237045\n",
            "epoch: 15 batch: 19 current batch loss: 0.04174347594380379\n",
            "epoch: 15 batch: 20 current batch loss: 0.04053466022014618\n",
            "epoch: 15 batch: 21 current batch loss: 0.049373775720596313\n",
            "epoch: 15 batch: 22 current batch loss: 0.04081717133522034\n",
            "epoch: 15 batch: 23 current batch loss: 0.05072638764977455\n",
            "epoch: 15 batch: 24 current batch loss: 0.04432237520813942\n",
            "epoch: 15 batch: 25 current batch loss: 0.03669474646449089\n",
            "epoch: 15 batch: 26 current batch loss: 0.029370510950684547\n",
            "epoch: 15 batch: 27 current batch loss: 0.036281000822782516\n",
            "epoch: 15 batch: 28 current batch loss: 0.0373300239443779\n",
            "epoch: 15 batch: 29 current batch loss: 0.02395632117986679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now reuse Anti-MNIST from hw9"
      ],
      "metadata": {
        "id": "mnvYLf7PhJ0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Anti_MNIST(torch.nn.Module):\n",
        "    def __init__ (self, trained_classifier):\n",
        "        super().__init__()\n",
        "\n",
        "        # Assure the model's weights are frozen\n",
        "        trained_classifier.eval()\n",
        "        for param in trained_classifier.parameters:\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        self.imgs = torch.nn.Parameter(torch.randn((10, 1, 28, 28)))\n",
        "        self.classifier = trained_classifier # Ensure this is trained!\n",
        "\n",
        "    def forward(self, _): # ignore x, we don't care\n",
        "        output = self.classifier(self.imgs)\n",
        "        return output"
      ],
      "metadata": {
        "id": "7Yrupj4ChdPS"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}